{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Capstone with Pytorch.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f0117f1ecae429da0c323dc76d1e963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_88b96ab779d64cb1abf477cd6e347c0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0a86f89c42a443f983e18d9338718b5c",
              "IPY_MODEL_b822c20a6e7f4518a9d0e000365eb396",
              "IPY_MODEL_567c78cd3d1c4840bde72fa97d86c851"
            ]
          }
        },
        "88b96ab779d64cb1abf477cd6e347c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a86f89c42a443f983e18d9338718b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10468055357a4d0c8fdcbef2771addc4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05a0108152b74e22aca4095ffa0388a1"
          }
        },
        "b822c20a6e7f4518a9d0e000365eb396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_69dfc8730ad14ee7a66524c3432df682",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a384ed41ebb6482fbd88bee4c17fac36"
          }
        },
        "567c78cd3d1c4840bde72fa97d86c851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_55719754a2e0407fb4cc328f81336e27",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [10:50&lt;00:00, 650.81s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94f33af82f4d4923a71f637bc768eb57"
          }
        },
        "10468055357a4d0c8fdcbef2771addc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05a0108152b74e22aca4095ffa0388a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69dfc8730ad14ee7a66524c3432df682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a384ed41ebb6482fbd88bee4c17fac36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55719754a2e0407fb4cc328f81336e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94f33af82f4d4923a71f637bc768eb57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEXYdz2LL0CX"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgMS-XsXL0Cc"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q0tATJ1L0Cc"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u9gWqQFL0Cd"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wji84XKdL0Ce"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e85yB64dL0Ce"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXrlNOyJL0Cf"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ9ZNMYlL0Cg"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y18X9u-qL0Cg",
        "outputId": "1906e717-07ed-4d29-97b7-0c684ab80fd5"
      },
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-18 16:17:05--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  36.0MB/s    in 74s     \n",
            "\n",
            "2021-10-18 16:18:20 (33.5 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9voP9BKrL0Cj"
      },
      "source": [
        "!unzip -q Positive_tensors.zip "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FScfJ5cfL0Cj",
        "outputId": "2b7c4417-061e-43d7-89c0-b66f50f8b824"
      },
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-18 16:22:27--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  36.4MB/s    in 59s     \n",
            "\n",
            "2021-10-18 16:23:27 (34.0 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CnSEHSmL0Ck"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL_wT_VfL0Ck",
        "outputId": "0d318447-fb17-404e-bdbf-b9229df4e0df"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjdB1zX4L0Ck"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqON8TzsL0Cl"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo0ZAwP2L0Cl",
        "outputId": "5e2507d9-f912-414b-85d1-59043f78e667"
      },
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f265339ced0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DYUHOjOL0Cm"
      },
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xts6ESpL0Cm"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX__cq7GL0Cm"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpCYzdD6L0Cn",
        "outputId": "3788bef5-38c3-48ba-8a69-9c1c4257f802"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqjauZDjL0Cn"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53AI-NeLL0Cn",
        "outputId": "37dbbd81-3152-4719-82f6-1b5d65b43e32"
      },
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0kQjFa8L0Co"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHfgcC2CL0Co",
        "outputId": "ad4333eb-24db-4335-d2a0-d20f778332d6"
      },
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thX5GdgiL0Co"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdakFsV6L0Co"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxk_z94pL0Co"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMHEooOdL0Cp"
      },
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "\n",
        "model = models.resnet18(pretrained=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztRYfmCgL0Cp"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3rn8zt9L0Cp"
      },
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.require_grad=False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP3WD6jyL0Cp"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp3dIjj4L0Cp"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unio6yyoL0Cp"
      },
      "source": [
        "model.fc = nn.Linear(512, 2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNG0Mi_aL0Cq"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50WD-vHaL0Cq",
        "outputId": "bc8e27c6-44cc-48e3-a633-e49829be8558"
      },
      "source": [
        "print(model)\n",
        "model.to(torch.device(\"cuda\"))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36hRFpipL0Cq"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46DDwT2JL0Cq"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLTTpaPsL0Cq"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnD1fpYlL0Cr"
      },
      "source": [
        "# Step 1: Create the loss function\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZGn2r3L0Cr"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnw8KfwgL0Cs"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100 )\n",
        "validation_loader = DataLoader(dataset=validation_dataset, batch_size=100 )"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXJQPrc3L0Cs"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kadCuU4_L0Cs"
      },
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Qfjod5L0Cs"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpX3LqOQL0Cs"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9f0117f1ecae429da0c323dc76d1e963",
            "88b96ab779d64cb1abf477cd6e347c0f",
            "0a86f89c42a443f983e18d9338718b5c",
            "b822c20a6e7f4518a9d0e000365eb396",
            "567c78cd3d1c4840bde72fa97d86c851",
            "10468055357a4d0c8fdcbef2771addc4",
            "05a0108152b74e22aca4095ffa0388a1",
            "69dfc8730ad14ee7a66524c3432df682",
            "a384ed41ebb6482fbd88bee4c17fac36",
            "55719754a2e0407fb4cc328f81336e27",
            "94f33af82f4d4923a71f637bc768eb57"
          ]
        },
        "id": "uB_DAEQiL0Cs",
        "outputId": "f37c8e5d-02e1-40cb-c01a-40c855fa4ce6"
      },
      "source": [
        "from tqdm.notebook import tnrange\n",
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in tnrange(n_epochs):\n",
        "    loss_sublist = []\n",
        "    for x, y in train_loader:\n",
        "        model.train() \n",
        "        #clear gradient\n",
        "        model.zero_grad()\n",
        "     \n",
        "        #make a prediction\n",
        "        z = model(x.to(torch.device(\"cuda\")))\n",
        "   \n",
        "        # calculate loss\n",
        "        loss = criterion(z, y.to(torch.device(\"cuda\")))\n",
        "        loss_sublist.append(loss.data.item())\n",
        "        # calculate gradients of parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.data)\n",
        "        \n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z = model(x_test.to(torch.device(\"cuda\")))\n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct +=(yhat.to(torch.device(\"cpu\"))==y_test).sum().item()\n",
        "   \n",
        "    accuracy=correct/N_test\n",
        "    accuracy_list.append(accuracy)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f0117f1ecae429da0c323dc76d1e963",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z31dQqmL0Ct"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SE4twugL0Ct",
        "outputId": "c0085361-5b12-4a38-a02f-ef3c881666dc"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9967"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "mevuZi9JL0Ct",
        "outputId": "109e3d06-37d5-4dff-98b4-e0c493373b08"
      },
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1L_cBRvL0Ct"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlnRV5yvL0Ct"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmkiErtFL0Ct",
        "outputId": "89898d9f-8bae-4e4e-8e4a-1af1028f3b76"
      },
      "source": [
        "count = 0\n",
        "for i, (x_test, y_test) in enumerate(validation_loader):\n",
        "      # set model to eval \n",
        "      model.eval()\n",
        "      #make a prediction \n",
        "      z = model(x_test.to(torch.device(\"cuda\")))\n",
        "      #find max \n",
        "      _, yhat = torch.max(z.data, 1)\n",
        "      \n",
        "      #Calculate misclassified  samples in mini-batch \n",
        "      #hint +=(yhat==y_test).sum().item()\n",
        "      yhat = yhat.to(torch.device(\"cpu\"))\n",
        "      misclassified = yhat!=y_test\n",
        "      if misclassified.sum().item() > 0:\n",
        "        indexes = misclassified.nonzero()\n",
        "\n",
        "        for id, yt, yh in zip(indexes, y_test[misclassified], yhat[misclassified]):\n",
        "          print(\"sample {} predicted value: {} actual value:{}\".format((i*100)+id.item(), yh, yt))\n",
        "          count += 1\n",
        "          if count > 3:\n",
        "            break\n",
        "        if count > 3:\n",
        "            break"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 199 predicted value: 1 actual value:0\n",
            "sample 225 predicted value: 1 actual value:0\n",
            "sample 476 predicted value: 0 actual value:1\n",
            "sample 504 predicted value: 0 actual value:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBv1GlFDL0Ct"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNwA-3W8L0Cu"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vfPSzoWL0Cu"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgBfaNdPL0Cu"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01\">MIT License</a>.\n"
      ]
    }
  ]
}